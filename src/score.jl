"""
    getcholesky(J::AbstractMatrix)

Cholesky decomposition of J, assumed to be symmetric *positive* definite,
stored as a `PDMat` object.
Warning: PDMat is not a subtype of Cholesky.
[PDMats.jl](https://github.com/JuliaStats/PDMats.jl) is efficient for
structured matrices (e.g diagonal or sparse) and has efficient methods for
linear algebra, e.g. `\`, `invquad`, `X_invA_Xt` etc.
"""
function getcholesky(J::AbstractMatrix)
    return PDMat(J) # LA.cholesky(b.J)
end
"""
    getcholesky_Œº(J::AbstractMatrix, h)
    getcholesky_Œº!(belief::Belief)

Tuple `(Jchol, Œº)` where `Jchol` is a cholesky representation of `J` or `belief.J`
and `Œº` is J‚Åª¬πh, used to update `belief.Œº` (by the second method).
"""
function getcholesky_Œº(J::AbstractMatrix, h)
    Jchol = getcholesky(J)
    Œº = Jchol \ h
    return (Jchol, Œº)
end
@doc (@doc getcholesky_Œº) getcholesky_Œº!
function getcholesky_Œº!(b::Belief)
    (Jchol, Œº) = getcholesky_Œº(b.J, b.h)
    b.Œº .= Œº
    return (Jchol, Œº)
end

"""
    entropy(J::Cholesky)
    entropy(J::AbstractMatrix)
    entropy(belief::AbstractBelief)

Entropy of a multivariate Gaussian distribution with precision matrix `J`,
assumed to be square and symmetric (not checked).
It is 0 if `J` is empty (of size 0√ó0). It may be `Inf` if `J` is semi-definite.
The second version applies the first to the belief precision `belief.J`.

`entropy` is defined for discrete distributions in
[StatsBase.jl](https://juliastats.org/StatsBase.jl/stable/scalarstats/#StatsBase.entropy)
and extended to Gaussian distributions in Distributions.jl around
[here](https://github.com/JuliaStats/Distributions.jl/blob/master/src/multivariate/mvnormal.jl#L95)
"""
function entropy(J::Union{LA.Cholesky{T},PDMat{T}}) where T<:Real
    n = size(J,2)
    n == 0 && return zero(T)
    (n * (T(log2œÄ) + 1) - LA.logdet(J)) / 2
end
function entropy(J::AbstractMatrix{T}) where T<:Real
    n = size(J,2)
    n == 0 && return zero(T)
    (n * (T(log2œÄ) + 1) - LA.logdet(LA.Symmetric(J))) / 2
end
entropy(cluster::AbstractBelief) = entropy(cluster.J)

"""
    average_energy!(ref::Belief, target::AbstractBelief, dropg::Bool=false)
    average_energy!(ref::Belief, J‚Çú, h‚Çú, g‚Çú)
    average_energy(J·µ£::Union{LA.Cholesky,PDMat}, Œº·µ£, J‚Çú, h‚Çú, g‚Çú)

Average energy (i.e. negative expected log) of a `target` canonical form with
parameters `(J‚Çú, h‚Çú, g‚Çú)` with respect to a normalized non-degenerate reference
canonical form `ref` with parameters `(J·µ£, h·µ£)`. The reference distribution
is normalized, so specifying `g·µ£` is unnecessary.
When the target canonical form is also normalized and non-degenerate,
this is equal to their cross-entropy:

    H(f·µ£, f‚Çú) = - E·µ£(log f‚Çú) = - ‚à´ f·µ£ log f‚Çú .

If `dropg=true`, then average energy is computed assuming that `g‚Çú=0`.
`ref` is assumed to be non-degenerate, that is, `J·µ£` should be positive definite.

`average_energy!` modifies the reference belief by updating `ref.Œº` to J‚Åª¬πh.
It calls `average_energy` after a cholesky decomposition of `ref.J`,
stored in `J·µ£`: see [`getcholesky_Œº!`](@ref).

## Calculation:

ref: f(x) = C(x | J·µ£, h·µ£, _) is the density of ùí©(Œº=J·µ£‚Åª¬πh·µ£, Œ£=J·µ£‚Åª¬π)  
target: C(x | J‚Çú, h‚Çú, g‚Çú) = exp( - (1/2)x'J‚Çúx - h‚Çú'x - g‚Çú )

    E[-log C(X | J‚Çú, h‚Çú, g‚Çú)] where X ‚àº C(J·µ£, h·µ£, _)
    = 0.5 (Œº·µ£'J‚Çú Œº·µ£ + tr(J·µ£‚Åª¬πJ‚Çú)) - h‚Çú'Œº·µ£ - g‚Çú

With empty vectors and matrices (J's of dimension 0√ó0 and h's of length 0),
the result is simply: - g‚Çú.
"""
function average_energy!(ref::Belief, target::AbstractBelief, dropg::Bool=false)
    g‚Çú = (dropg ? zero(target.g[1]) : target.g[1])
    average_energy!(ref, target.J, target.h, g‚Çú)
end
function average_energy!(ref::Belief, J‚Çú, h‚Çú, g‚Çú)
    (J·µ£, Œº·µ£) = getcholesky_Œº!(ref)
    average_energy(J·µ£, Œº·µ£, J‚Çú, h‚Çú, g‚Çú)
end
@doc (@doc average_energy) average_energy!
function average_energy(J·µ£::Union{LA.Cholesky,PDMat}, Œº·µ£, J‚Çú, h‚Çú, g‚Çú)
    isempty(J‚Çú) && return -g‚Çú # dot(x,A,x) fails on empty x & A
    (LA.tr(J·µ£ \ J‚Çú) + LA.dot(Œº·µ£, J‚Çú, Œº·µ£)) / 2 - LA.dot(h‚Çú, Œº·µ£) - g‚Çú
end

"""
    factored_energy(beliefs::ClusterGraphBelief)

Factored energy functional for general cluster graphs (Koller & Friedman 2009),
which approximates the evidence lower bound (ELBO), a lower bound for the
log-likelihood. It is
also called the (negative) Bethe free energy in the context of factor graphs
It is the sum of the cluster average energies and entropies,
minus the sepset entropies.
It is assumed but not checked that `beliefs` are calibrated
(neighbor clusters and sepset beliefs are consistent, used as local marginals).

For a calibrated clique tree, the factored energy is equal to the
log-likelihood. For a calibrated cluster graph, it can serve as as approximation.

output: tuple of 3 values, the 3rd being the factored energy:
(average energy, approximate entropy, factored energy = -energy + entropy).

See also: [`free_energy`](@ref),
[`entropy`](@ref),
[`average_energy!`](@ref),
[`iscalibrated`](@ref)

## References

D. Koller and N. Friedman.
*Probabilistic graphical models: principles and techniques*.
MIT Press, 2009. ISBN 9780262013192.

D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: A Review
for Statisticians, Journal of the American statistical Association, 112:518,
859-877, 2017, doi: [10.1080/01621459.2017.1285773](https://doi.org/10.1080/01621459.2017.1285773).
"""
function factored_energy(b::ClusterGraphBelief)
    res = free_energy(b)
    return (res[1], res[2], -res[3])
end

"""
    free_energy(beliefs::ClusterGraphBelief)

negative [`factored_energy`](@ref) to approximate the negative log-likelihood.
The approximation is exact on a clique tree after calibration.
"""
function free_energy(beliefs::ClusterGraphBelief{B}) where B<:Belief{T} where T<:Real
    b = beliefs.belief
    init_b = beliefs.factor
    nclu = nclusters(beliefs)
    ave_energy = zero(T)
    approx_entropy = zero(T)
    for i in 1:nclu
        fac = init_b[i]
        if isempty(fac.J) # then b[i].J should be empty too
            ave_energy -= fac.g[1]
        else # do 1 cholesky of b[i], use it twice
            (Jclu, Œºclu) = getcholesky_Œº!(b[i])
            ave_energy += average_energy(Jclu, Œºclu, fac.J, fac.h, fac.g[1])
            approx_entropy += entropy(Jclu)
        end
    end
    for i in (nclu+1):length(b)
        approx_entropy -= entropy(b[i])
    end
    return (ave_energy, approx_entropy, ave_energy - approx_entropy)
end
